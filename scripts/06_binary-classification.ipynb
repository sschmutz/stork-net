{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification of stork nest images\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sschmutz/stork-net/blob/master/scripts/05_binary-classification.ipynb)  \n",
    "To use Google Colab, click on the link above and then change the Runtime type to Python 3 under \"Runtime\" - \"Change runtime type\". And for faster computation select GPU under \"Hardware accelerator\".\n",
    "\n",
    "Code is adapted from the [TensorFlow Tutorial on Image classification](https://www.tensorflow.org/tutorials/images/classification).\n",
    "\n",
    "The goal is to classify images of a stork nest in two categories, if a stork is present or not. The images were collected from a publicly available [webcam](https://www.berner-storch.ch/webcam/) and manually labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images\n",
    "Labeled images are already split and [available on GitHub](https://github.com/sschmutz/stork-net-dataset).\n",
    "The full dataset will be downloaded with the following comand, this enables us to use this notebook in google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = tf.keras.utils.get_file(origin=\"https://github.com/sschmutz/stork-net-dataset/archive/master.zip\", fname=\"stork-net-dataset-master.zip\", extract=True)\n",
    "data_dir = pathlib.Path(os.path.splitext(data_dir)[0])\n",
    "\n",
    "train_dir = pathlib.Path(data_dir, \"2019_train\", \"train\")\n",
    "validation_dir = pathlib.Path(data_dir, \"2019_train\", \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_0_stork_train = len(list(train_dir.glob(\"0_stork/*.jpg\")))\n",
    "num_1_stork_train = len(list(train_dir.glob(\"1_stork/*.jpg\")))\n",
    "num_2_stork_train = len(list(train_dir.glob(\"2_stork/*.jpg\")))\n",
    "num_3_stork_train = len(list(train_dir.glob(\"3_stork/*.jpg\")))\n",
    "\n",
    "num_0_stork_val = len(list(validation_dir.glob(\"0_stork/*.jpg\")))\n",
    "num_1_stork_val = len(list(validation_dir.glob(\"1_stork/*.jpg\")))\n",
    "num_2_stork_val = len(list(validation_dir.glob(\"2_stork/*.jpg\")))\n",
    "num_3_stork_val = len(list(validation_dir.glob(\"3_stork/*.jpg\")))\n",
    "\n",
    "total_train = len(list(train_dir.glob(\"*/*.jpg\")))\n",
    "total_val = len(list(validation_dir.glob(\"*/*.jpg\")))\n",
    "\n",
    "class_names = np.array([item.name for item in train_dir.glob(\"*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not sure if the numbers have to be divisible by the batch size.\n",
    "# Currently the batch size is chosen to be divisible by total_train (280) and total_val (60)\n",
    "batch_size = 20 \n",
    "epochs = 15\n",
    "img_height = 480\n",
    "img_width = 640\n",
    "channels = 3 #set to 1 if greyscale is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation can be defined already inside ***ImageDataGenerator()***, see the respective section on the [keras website](https://keras.io/api/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1./255 is to convert from uint8 to float32 in range [0,1]\n",
    "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we change the color-images to grayscale? This way one can maybe use images from the infrared camera at night.\n",
    "This could be done in ***flow_from_directory()***, just define following parameter: ***color_mode=\"grayscale\"*** (default is \"rgb\").\n",
    "\n",
    "If we use a multiclass-classification problem later on, we can define ***class_mode=\"categorical\"***. Labels will be automatically be 2D one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(img_height, img_width),\n",
    "                                                           class_mode=\"binary\",\n",
    "                                                           classes = list(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                              directory=validation_dir,\n",
    "                                                              target_size=(img_height, img_width),\n",
    "                                                              class_mode=\"binary\",\n",
    "                                                              classes = list(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_images, _ = next(train_data_gen)\n",
    "sample_validation_images, _ = next(val_data_gen)\n",
    "\n",
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"0_stork\", \"1_stork\", \"2_stork\", \"3_stork\"]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(sample_training_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    #plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding=\"same\", activation=\"relu\", input_shape=(img_height, img_width, channels)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(4)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "loss=history.history[\"loss\"]\n",
    "val_loss=history.history[\"val_loss\"]\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_acc, label=\"Validation Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_prob = tf.round(tf.nn.sigmoid(predictions))\n",
    "predictions_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 4, figsize=(24, 40))\n",
    "axes = axes.flatten()\n",
    "for img, ax, i in zip(sample_validation_images, axes, range(60)):\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(round(predictions.item(i)))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
